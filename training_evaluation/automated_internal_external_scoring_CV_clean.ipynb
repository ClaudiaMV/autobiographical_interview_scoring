{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "automated_internal_external_scoring_CV_clean.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP3iGE2018IHaYzFu8kNG36",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubenvangenugten/autobiographical_interview_scoring/blob/main/automated_internal_external_scoring_CV_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is used to train with leave-one-dataset out crossvalidation.\n",
        "\n",
        "All code for model training, evaluation, preparing data for training, and related matters has been copied or adapted from the huggingface [documentation](https://huggingface.co/docs/transformers/index) and the example [library](https://github.com/huggingface/notebooks/blob/master/transformers_doc/training.ipynb)\n"
      ],
      "metadata": {
        "id": "GHbVWJ0snwZv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56m1-IXxdwf7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC0eYIzcfSkJ"
      },
      "source": [
        "pip install transformers==4.6.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H38I2yHWfxoo"
      },
      "source": [
        "pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IisHACG0e26J"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "from datasets import load_metric\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "pd.set_option('display.max_rows', 20) # Display 20 rows\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred[:2]\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "def findClosest(num, collection):\n",
        "   return min(collection,key=lambda x:abs(x-num))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in data.\n",
        "# This data has one sentence per row, so that bert can be trained.\n",
        " \n",
        "allDat = pd.read_csv('/content/drive/MyDrive/automated_internal_external_scoring_CV_example/synthetic_data_training_repeat.csv')\n"
      ],
      "metadata": {
        "id": "xJxEzD7T0oVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niXS-fhvY_CS"
      },
      "source": [
        "allDat = allDat[- allDat.Type.isnull()] # remove rows without data\n",
        "\n",
        "allDat[\"percentInt_sentence\"] = allDat.numInt_sentence/allDat.numTotal_sentence\n",
        "\n",
        "percentageList = [0, .5, .75, 1]\n",
        "\n",
        "allDat[\"closestPercentage\"] = [findClosest(i, percentageList) for i in allDat.percentInt_sentence]\n",
        "\n",
        "\n",
        "# The code below uses \"Type\" as the variable that has the labels.\n",
        "# so assign closest percentage to type.\n",
        "\n",
        "# so, the type column that previously contained 'external', 'intenral', and 'mixed'\n",
        "# will now be replaced with the items in percentageList\n",
        "\n",
        "allDat.loc[:,[\"Type\"]] = allDat[\"closestPercentage\"]\n",
        "\n",
        "# now, since the transformer expects integers, recast the percentage list to integer choices\n",
        "\n",
        "conditions = [\n",
        "    (allDat[\"closestPercentage\"] == 0),\n",
        "    (allDat[\"closestPercentage\"] == .5),\n",
        "    (allDat[\"closestPercentage\"] == .75),\n",
        "    (allDat[\"closestPercentage\"] == 1)\n",
        "]\n",
        "choices = [0, 1, 2, 3]\n",
        "\n",
        "allDat[\"Type\"] = np.select(conditions, choices)\n",
        "\n",
        "# can also remove sentneces that have more than 8 details, since that seems excessively \n",
        "# long (e.g. a full narrative)\n",
        "\n",
        "allDat = allDat[allDat.numTotal_sentence <= 8]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzsBvGWAiKEt"
      },
      "source": [
        "os.chdir('/content/drive/MyDrive/automated_internal_external_scoring_CV_example/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7-FxvysZbAc"
      },
      "source": [
        "##### start the process of leave-one-dataset-out validation\n",
        "\n",
        "def runDatasetCV(testDatasetName):\n",
        "    \n",
        "    # now, split up data into training and test.\n",
        "\n",
        "    trainData = allDat[allDat.study != testDatasetName]\n",
        "    testData = allDat[allDat.study == testDatasetName]\n",
        "    \n",
        "    # remove opinions and description tasks, since those have different scoring procedures\n",
        "    trainData = trainData[trainData.task != 'Description']\n",
        "    trainData = trainData[trainData.task != 'Opinion']\n",
        "\n",
        "    # now, upsample the training data to the highest value\n",
        "    \n",
        "    training_num_sentences = Counter(trainData.Type)\n",
        "    max_training_num_sentences = max(training_num_sentences.values())\n",
        "      \n",
        "    types = trainData.Type.unique()\n",
        "\n",
        "    for sentenceType in types:\n",
        "        thisType = sentenceType\n",
        "        trainData_thisType = trainData[trainData.Type == thisType]\n",
        "        \n",
        "        upsampling_how_many_add = max_training_num_sentences - trainData_thisType.shape[0]\n",
        "        \n",
        "        addedData = trainData_thisType.sample(n = upsampling_how_many_add,\n",
        "                                              replace = True,\n",
        "                                              random_state = 2021,\n",
        "                                              axis = 0)\n",
        "        trainData = trainData.append(addedData)\n",
        "        \n",
        "    training_num_sentences_afterResample = Counter(trainData.Type)\n",
        "    \n",
        "    # data is now upsampled    \n",
        "    # now, make sure data are character\n",
        "    \n",
        "    trainData.loc[:,'sentence'] = trainData.loc[:,'sentence'].astype('str')\n",
        "    testData.loc[:,'sentence'] = testData.loc[:,'sentence'].astype('str')\n",
        "                \n",
        "    \n",
        "    #  create lists that bert can work with. \n",
        "    #  training set:\n",
        "    \n",
        "    texts = []\n",
        "    labels = []\n",
        "    \n",
        "    for row in range(trainData.shape[0]):\n",
        "        \n",
        "        temp = trainData.iloc[row, trainData.columns.get_loc(\"sentence\")]\n",
        "        temp = str(temp) # strip name of dataframe, then turn into string\n",
        "        texts.append(temp)\n",
        "        \n",
        "        temp2 = trainData.iloc[row, trainData.columns.get_loc(\"Type\")]\n",
        "            \n",
        "        labels.append(temp2)\n",
        "        \n",
        "    # testing set:\n",
        "        \n",
        "    texts_test = []\n",
        "    labels_test = []\n",
        "    \n",
        "    for row2 in range(testData.shape[0]):\n",
        "        \n",
        "        temp_test = testData.iloc[row2, testData.columns.get_loc(\"sentence\")]\n",
        "        temp_test = str(temp_test) # strip name of dataframe, then turn into string\n",
        "        texts_test.append(temp_test)\n",
        "        \n",
        "        temp_test2 = testData.iloc[row2, testData.columns.get_loc(\"Type\")]\n",
        "    \n",
        "        labels_test.append(temp_test2)\n",
        "    \n",
        "    # now shuffle the  training labels/lists together so that bert \n",
        "    # doesnt see them in any particular order\n",
        "    \n",
        "    temp = list(zip(texts, labels))\n",
        "    random.shuffle(temp)\n",
        "    texts, labels = zip(*temp)\n",
        "    \n",
        "    # just for naming:\n",
        "        \n",
        "    test_texts = texts_test\n",
        "    test_labels = labels_test\n",
        "    train_texts = texts\n",
        "    train_labels = labels\n",
        "    \n",
        "    \n",
        "    # now, split for training/val\n",
        "    \n",
        "    train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)\n",
        "    \n",
        "    train_texts = list(train_texts) # unsure why train list is a tple, but cast to list\n",
        "    train_labels = list(train_labels) # unsure why train list is a tple, but cast to list\n",
        "    \n",
        "    \n",
        "    # load tokenizer\n",
        "    \n",
        "    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')    \n",
        "    \n",
        "    # encode text into something that bert can work with\n",
        "    \n",
        "    train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "    val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "    test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
        "    \n",
        "    # create dataset so that bert can work with it\n",
        "    \n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        dict(train_encodings),\n",
        "        train_labels\n",
        "    ))\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        dict(val_encodings),\n",
        "        val_labels\n",
        "    ))\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        dict(test_encodings),\n",
        "        test_labels\n",
        "    ))\n",
        "    \n",
        "    # set up model\n",
        "    \n",
        "    model_outputDir = './results_CVTraining_' + testDatasetName + 'Testing'\n",
        "    \n",
        "    training_args = TFTrainingArguments(\n",
        "        output_dir=model_outputDir,          # output directory\n",
        "        num_train_epochs=3,              # total number of training epochs\n",
        "        per_device_train_batch_size=16,  # batch size per device during training\n",
        "        per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "        weight_decay=0.01,               # strength of weight decay\n",
        "        logging_dir='./logs',            # directory for storing logs\n",
        "        logging_steps=10,\n",
        "    )\n",
        "    \n",
        "    with training_args.strategy.scope():\n",
        "        model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4)\n",
        "    \n",
        "    trainer = TFTrainer(\n",
        "        model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "        args=training_args,                  # training arguments, defined above\n",
        "        train_dataset=train_dataset,         # training dataset\n",
        "        eval_dataset=val_dataset             # validation dataset\n",
        "    )\n",
        "    \n",
        "    # now train it!\n",
        "    \n",
        "    trainer.train()\n",
        "    \n",
        "    # evaluate it to see how it did on the validations set\n",
        "    trainer.evaluate()\n",
        "    \n",
        "    # let's see how it does on test data.\n",
        "    \n",
        "    preds_test = trainer.predict(test_dataset)\n",
        "    \n",
        "    \n",
        "    # let's calculate performance metrics   \n",
        "    # compute_metrics(preds_test)\n",
        "    \n",
        "    logits, labels = preds_test[:2]\n",
        "    \n",
        "    predictions = np.argmax(logits, axis=-1)    \n",
        "    testData[['predictions']] = predictions\n",
        "\n",
        "    #confusion_matrix(labels, predictions)\n",
        "            \n",
        "    \n",
        "    # now, calculate the amount of internal content per line\n",
        "    # as well as the amount of external content\n",
        "    \n",
        "    testData[['numInt_preds']] = 0\n",
        "    testData[['numExt_preds']] = 0\n",
        "    \n",
        "    # now loop through each row and add in the coutns\n",
        "    \n",
        "    for row in range(testData.shape[0]):\n",
        "        \n",
        "        predictionType_thisIter = testData.iloc[row, testData.columns.get_loc(\"predictions\")]\n",
        "       \n",
        "        internalLocation = testData.columns.get_loc(\"numInt_preds\")\n",
        "        externalLocation = testData.columns.get_loc(\"numExt_preds\")\n",
        "    \n",
        "        \n",
        "        if predictionType_thisIter == 0:\n",
        "            testData.iloc[row, externalLocation] = \\\n",
        "                testData.iloc[row, testData.columns.get_loc(\"sentenceWordCount\")]\n",
        "        \n",
        "        if predictionType_thisIter == 1:\n",
        "            numTotalDetails = testData.iloc[row, testData.columns.get_loc(\"sentenceWordCount\")]\n",
        "            halfDetails = numTotalDetails/2\n",
        "            testData.iloc[row, externalLocation] = halfDetails\n",
        "            testData.iloc[row, internalLocation] = halfDetails\n",
        "    \n",
        "        if predictionType_thisIter == 2:\n",
        "            numTotalDetails = testData.iloc[row, testData.columns.get_loc(\"sentenceWordCount\")]        \n",
        "            testData.iloc[row, externalLocation] = numTotalDetails/4\n",
        "            testData.iloc[row, internalLocation] = numTotalDetails*(3/4)\n",
        "                \n",
        "        if predictionType_thisIter == 3:\n",
        "            testData.iloc[row, internalLocation] = \\\n",
        "                testData.iloc[row, testData.columns.get_loc(\"sentenceWordCount\")]\n",
        "      \n",
        "    # write out the predictions for the test set. save model.\n",
        "    testData_outputName = 'testData_' + testDatasetName + '_CVTraining.csv'\n",
        "\n",
        "    testData.to_csv(testData_outputName)\n",
        "    \n",
        "    save_directory = os.getcwd() + '/bert_finetuned_oversampled/' + '_CVTraining_' + testDatasetName\n",
        "    \n",
        "    trainer.save_model(save_directory)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNnNL1U1ZW-9"
      },
      "source": [
        "allStudies = allDat.study.unique()\n",
        "\n",
        "for studyName in allStudies:\n",
        "    runDatasetCV(studyName)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}

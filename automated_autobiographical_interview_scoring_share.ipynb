{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "automated_autobiographical_interview_scoring_share.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNs8aJ6AMoK6xLrwaYBfCA8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubenvangenugten/autobiographical_interview_scoring/blob/main/automated_autobiographical_interview_scoring_share.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome!"
      ],
      "metadata": {
        "id": "4hUhMXYd1NwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please read through the .readme file on github to ensure your data is in the correct format. \n",
        "\n",
        "\n",
        "To use this code, you'll first have to tell colab where to find your data. You can find the location of your data folder on google drive by clicking (single-click) on the folder and looking at the bottom of your screen. At the bottom, you should see something like:\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "The folder location is '/content/drive/' plus whatever is at the bottom of the screen. So, in this case, '/content/drive/MyDrive/automated_scoring_example/'"
      ],
      "metadata": {
        "id": "C5MXPj4bi8gi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Edit the cell below to provide the data file location and the location where you want the results stored.\n"
      ],
      "metadata": {
        "id": "BBhJIUn31-dU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_location = '/content/drive/MyDrive/automated_scoring_example/example.csv'\n",
        "output_location = '/content/drive/MyDrive/automated_scoring_example/'"
      ],
      "metadata": {
        "id": "U6ZpPMQFynH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, to run the code, click 'runtime' -> 'run all' in your menu bar. You'll be prompted for google drive login, which colab needs to access your file. Once the code is done running, you should be able to find the results in the location you set in output_location"
      ],
      "metadata": {
        "id": "UEbINSdszZxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unless you want to customize something, you should not have to edit the code below."
      ],
      "metadata": {
        "id": "oMIBtaqmyyDh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell below allows colab to access google drive"
      ],
      "metadata": {
        "id": "YS7CujXlz_9k"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEOQBui2gHPw",
        "outputId": "927bd42c-56a7-4ac0-f3cd-d9dbe077bb27"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below installs necessary libraries"
      ],
      "metadata": {
        "id": "FFmEQYm10Dr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers==4.6.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sf7YZtJq1Pmd",
        "outputId": "8ed6d50d-b9c6-4517-f0ca-6c4693b937e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.6.0 in /usr/local/lib/python3.7/dist-packages (4.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (3.4.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (0.0.47)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (21.3)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (4.10.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.6.0) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.6.0) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.0) (3.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhS3DLo8iH3k",
        "outputId": "63f677cd-eb33-47c5-97bf-1d8544c59cff"
      },
      "source": [
        "pip install pysbd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pysbd in /usr/local/lib/python3.7/dist-packages (0.3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLEY26WHiKtS"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pysbd\n",
        "import re\n",
        "from transformers import AutoTokenizer, TFDistilBertForSequenceClassification\n",
        "from transformers import TextClassificationPipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "define sentence segmenter so that it's easier to use later\n"
      ],
      "metadata": {
        "id": "fOriBIr83JKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seg = pysbd.Segmenter(language=\"en\", clean=False)"
      ],
      "metadata": {
        "id": "Seobe6pJs6hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reload model trained on all datasets. \n",
        "Reload tokenizer saved with it (just standard distilbert tokenizer). These are stored on the huggingface model hub and can be accessed directly with code below"
      ],
      "metadata": {
        "id": "2L8qfiS7syWJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6lDKbVJlEQ-",
        "outputId": "c44bdd37-13c7-48ec-b759-f742cfd98f3a"
      },
      "source": [
        "model = TFDistilBertForSequenceClassification.from_pretrained(\"vangenugtenr/autobiographical_interview_scoring\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vangenugtenr/autobiographical_interview_scoring\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at vangenugtenr/autobiographical_interview_scoring were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at vangenugtenr/autobiographical_interview_scoring and are newly initialized: ['dropout_39']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read in data from the location provided at the top of this notebook"
      ],
      "metadata": {
        "id": "sGrJhywUs_ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "allDat = pd.read_csv(data_location)"
      ],
      "metadata": {
        "id": "jONHybF0biWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, take each narrative and split it up into sentences. We do this because sentences are the level at which we classify content. We want to end up with a new dataframe in which each row contains participantID, prompt, text, and one  sentence. So let's say we start with two initial rows of data: two narratives. Let's say that each has ten sentences. After the code below, our resulting dataframe should be 20 rows long.\n",
        "\n",
        "This way, the sentences can easily be classified and are matched to the stories they came from.\n",
        "\n",
        "Approach:\n",
        "\n",
        "Loop through each narrative, create a new dataframe where each row contains the participantID, prompt, text, and one of the resulting sentences. Store this dataframe. At the end, bind all dataframes together. This way, we obtain a dataframe with all narratives in long format, where each row has a sentence that can be classified."
      ],
      "metadata": {
        "id": "SPXmED7a3Jgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_dataframes = []\n",
        "\n",
        "for row in range(allDat.shape[0]):\n",
        "    \n",
        "    # access some general info about this narrative\n",
        "    this_subID = allDat.iloc[row, allDat.columns.get_loc(\"participantID\")]\n",
        "    this_prompt = allDat.iloc[row, allDat.columns.get_loc(\"prompt\")]\n",
        "    narrative = allDat.iloc[row, allDat.columns.get_loc(\"text\")]\n",
        "\n",
        "    # store current row\n",
        "    currentRow = allDat.iloc[[row], :] # if don't have brackets around row, not returned as Df, which is needed for merge\n",
        "\n",
        "    # create new dataframe with each row a new sentence, and subID and prompt added\n",
        "    segmented_sentences = seg.segment(narrative)\n",
        "    sentences_df = pd.DataFrame(segmented_sentences, columns=['sentence'])\n",
        "    sentences_df[\"participantID\"] = this_subID\n",
        "    sentences_df[\"prompt\"] = this_prompt\n",
        "\n",
        "    # create a new merged dataframe \n",
        "    merged_thisNarrative = pd.merge(currentRow, sentences_df, on=[\"participantID\", \"prompt\"])\n",
        "\n",
        "    list_of_dataframes.append(merged_thisNarrative)\n",
        "  \n",
        "\n",
        "testData = pd.concat(list_of_dataframes)\n"
      ],
      "metadata": {
        "id": "mKWDRVhlxCdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, prepare the data so that it can passed to our model"
      ],
      "metadata": {
        "id": "lza9Gqu9nPif"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yak_reqzosQ4"
      },
      "source": [
        "# now, make sure data are character\n",
        "\n",
        "testData.loc[:,'sentence'] = testData.loc[:,'sentence'].astype('str')\n",
        "        \n",
        "# create list of texts to classify (put in list format to encode texts)\n",
        "    \n",
        "test_texts = []\n",
        "\n",
        "for row2 in range(testData.shape[0]):\n",
        "    temp_test = testData.iloc[row2, testData.columns.get_loc(\"sentence\")]\n",
        "    temp_test = str(temp_test) # strip name of dataframe, then turn into string\n",
        "    test_texts.append(temp_test)\n",
        "\n",
        "\n",
        "# encode text into something that bert can work with.\n",
        "\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings)\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classify each sentence"
      ],
      "metadata": {
        "id": "E1z0ov7Cx-Tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up text classification pipeline using our model and tokenizer\n",
        "\n",
        "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n",
        "\n",
        "# to keep amount of ram low, so that we can use the free version of\n",
        "# google colab, split classification up into batches rather than all\n",
        "# sentences at once.\n",
        "\n",
        "stored_test = []\n",
        "batch_size = 200  \n",
        "\n",
        "for i in range(0, len(test_texts), batch_size):\n",
        "   stored_test.extend(pipe(test_texts[i:i+batch_size]))"
      ],
      "metadata": {
        "id": "VmV3-VMq6Tze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the step above, predicted labels are stored in a list format. We'd like to work with a dataframe of predictions. So just loop through each list and turn it into a dataframe, then put them all together"
      ],
      "metadata": {
        "id": "CKl3ubKqtgPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_predictionDfs = []\n",
        "\n",
        "for row in range(len(stored_test)):\n",
        "    \n",
        "    thisTestLabels = pd.DataFrame(stored_test[row]) \n",
        "    thisTestLabels.index = thisTestLabels['label']\n",
        "    thisTestLabels = thisTestLabels.drop('label', axis = 1)\n",
        "    thisTestLabels = thisTestLabels.transpose()\n",
        "\n",
        "    list_of_predictionDfs.append(thisTestLabels)\n",
        "\n",
        "predictionsDf = pd.concat(list_of_predictionDfs)"
      ],
      "metadata": {
        "id": "xCVdG5Vy8B3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identify which label was most likely for the text"
      ],
      "metadata": {
        "id": "a-yx-HvGt7FP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictionsDf['toplabel'] = predictionsDf.idxmax(axis=1)"
      ],
      "metadata": {
        "id": "mkHMdjCB_3Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge predictions dataframe with the long dataframe to obtain one large dataframe"
      ],
      "metadata": {
        "id": "CMlFm8dyt_gX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testData2 = pd.concat([testData.reset_index(drop=True), predictionsDf.reset_index(drop=True)], axis=1)"
      ],
      "metadata": {
        "id": "lwPS0kfcEtTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add a wordcount for each sentence. use for loop for readability (plus, takes no time)"
      ],
      "metadata": {
        "id": "R5mslp0BuJRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testData2[['sentenceWordCount']] = 0\n",
        "\n",
        "for row in range(testData2.shape[0]):\n",
        "  line = testData2.iloc[row, testData2.columns.get_loc(\"sentence\")]\n",
        "  count = len(re.findall(r'\\w+', line))\n",
        "  testData2.iloc[row, testData2.columns.get_loc(\"sentenceWordCount\")] = count"
      ],
      "metadata": {
        "id": "qYjijwlPJK72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create two new columns: number of words classified as internal, and number of words classified as external. Fill in those columns. As a reminder from the paper, each sentence is classified as containing 0% internal data (i.e. 100% external data), 50% internal, 75% internal, and 100% internal data."
      ],
      "metadata": {
        "id": "K20VsSncuNOE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeaY2X1ioBaM"
      },
      "source": [
        "testData2[['numInt_preds']] = 0\n",
        "testData2[['numExt_preds']] = 0\n",
        "\n",
        "# now loop through each row and add in the counts\n",
        "\n",
        "for row in range(testData2.shape[0]):\n",
        "    \n",
        "    predictionType_thisIter = testData2.iloc[row, testData2.columns.get_loc(\"toplabel\")]\n",
        "    numTotalWords = testData2.iloc[row, testData2.columns.get_loc(\"sentenceWordCount\")]\n",
        "\n",
        "    internalLocation = testData2.columns.get_loc(\"numInt_preds\")\n",
        "    externalLocation = testData2.columns.get_loc(\"numExt_preds\")\n",
        "    \n",
        "    if predictionType_thisIter == 'LABEL_0':\n",
        "        testData2.iloc[row, externalLocation] = numTotalWords\n",
        "\n",
        "    if predictionType_thisIter == 'LABEL_1':\n",
        "        halfDetails = numTotalWords/2\n",
        "        testData2.iloc[row, externalLocation] = halfDetails\n",
        "        testData2.iloc[row, internalLocation] = halfDetails\n",
        "\n",
        "    if predictionType_thisIter == 'LABEL_2':\n",
        "        testData2.iloc[row, externalLocation] = numTotalWords/4\n",
        "        testData2.iloc[row, internalLocation] = numTotalWords*(3/4)\n",
        "            \n",
        "    if predictionType_thisIter == 'LABEL_3':\n",
        "        testData2.iloc[row, internalLocation] = numTotalWords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have the predicted number of internal and exteranl words for each sentence. We are interested in a narrative-level summary. So, have to sum up all internal words per narrative and all external words per narrative. We do that below"
      ],
      "metadata": {
        "id": "VHqs4mPWq-Ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_write_out_subset = testData2.loc[:,[\"participantID\",\"prompt\", \"text\",\"numInt_preds\", \"numExt_preds\", 'sentenceWordCount']]\n",
        "grouped = test_write_out_subset.groupby(by = [\"participantID\", \"prompt\"]).agg({'text': 'first', \n",
        "                                            'numInt_preds': 'sum', \n",
        "                                            'numExt_preds': 'sum',\n",
        "                                            'sentenceWordCount': 'sum'})"
      ],
      "metadata": {
        "id": "xE9cei_Rn7jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we have summed across sentences, the sentenceWordCount column actually contains the wordcount of the whole narrative. so, just rename sentenceWordCount to totalWordCount, which is more appropriate"
      ],
      "metadata": {
        "id": "D1zwn6kOu3WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped.rename(columns = {\"sentenceWordCount\": \"totalWordCount\"}, \n",
        "          inplace = True)"
      ],
      "metadata": {
        "id": "mABWnBq9r5Hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, write out data to location specified at the top"
      ],
      "metadata": {
        "id": "HerHNIdTu82s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped.to_csv(output_location + '/automated_autobio_scored.csv')"
      ],
      "metadata": {
        "id": "jzAXj5PzLaBf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}